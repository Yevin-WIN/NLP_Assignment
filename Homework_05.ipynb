{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Homework 5: Recurrent Neural Networks\n",
    "\n",
    "#### Introduction to Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hyerin, Seo. (hyseo@students.uni-mainz.de)\n",
    "* Yeonwoo, Nam. (yeonam@students.uni-mainz.de)\n",
    "* Yevin, Kim. (kyevin@students.uni-mainz.de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this homework we're going to look at Syntactic Constituency Parsing with Recurrent Neural Networks (RNNs).\n",
    "\n",
    "*Total Points: 20P*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\anaconda3\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "# For this Exercise, we need scikit-learn\n",
    "# Install if missing\n",
    "import sys\n",
    "!{sys.executable} -m pip install scikit-learn\n",
    "\n",
    "# Or if this doesn't work, you can install it via your terminal in your anaconda environment\n",
    "# pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "*TASK 1: Describe the above dataset! (1P)* -> Evaluation: **XX/1**\n",
    "\n",
    "*TASK 2: Describe how you can develop a training approach using a sequence-to-sequence approach. (2P)* -> Evaluation: **XX/2**\n",
    "\n",
    "*TASK 3: What issues do you think can arrise from a sequence to sequence approach for this task? (1P)* -> Evaluation: **XX/1**\n",
    "\n",
    "*TASK 4: Explain what a sequence labeling task is! (3P)* -> Evaluation: **XX/2**\n",
    "\n",
    "*TASK 5: Write relative scale encoding for the following input! (1P)* -> Evaluation: **XX/2**\n",
    "\n",
    "*TASK 6:  Now write a function that converts the tree to a linearized tree in an relative scale encoding. (2P)* -> Evaluation: **XX/2**\n",
    "\n",
    "*TASK 7: Create Class Labels. Fill in sentences, encodings and labels variables. (2P)* -> Evaluation: **XX/2**\n",
    "\n",
    "*TASK 8: Build a PyTorch Dataset where the input consists of GloVe word indices, and utilize the class indices generated in the previous task as labels. (2P)* -> Evaluation: **XX/2**\n",
    "\n",
    "*TASK 9: Create a bidirectional RNN that initialized the embedding with the GloVe embedding! (2P)* -> Evaluation: **XX/2**\n",
    "\n",
    "*Task 10: Now create a training loop and train on your created dataloader! Print out the loss after each epoch. (3P)* -> Evaluation: **XX/3**\n",
    "\n",
    "*Task 11: Now, test your bidirectional RNN! Use accuracy as a metric. (1P)* -> Evaluation: **XX/1**\n",
    "\n",
    "**Total: XX/20**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Syntactic Constituency Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will explore syntactic constituency parsing.\n",
    "\n",
    "Syntactic constituency parsing: Breaking down a sentence into its basic parts (like nouns and verbs) and showing how they fit together to form the sentence's structure - Expressed in a tree structure (most of the time). It helps understand the grammatical makeup of a sentence.\n",
    "\n",
    "**Example**\n",
    "\n",
    "<img src=\"05_example2.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "From Wikipedia: \"**Syntactic parsing** is the automatic _analysis of syntactic structure_ of natural language, especially syntactic relations (in dependency grammar) and labelling spans of constituents (in constituency grammar).\"\n",
    "\n",
    "From Speech and Language Processing. Daniel Jurafsky & James H. Martin: \"The fundamental notion underlying the idea of **constituency** is that of abstraction — groups of words behaving as single units, or constituents. A significant part of developing a grammar involves discovering the inventory of constituents present in the language.\" \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Penn Treebank Dataset for our exercise sheet. See https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html for explanation of the tags.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP-SBJ (DT This)) (VP (VBZ is) (NP-PRD (NNP Japan))) (. ?))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "    \n",
    "# Download the Penn Treebank datas\n",
    "treebank_trees = treebank.parsed_sents()\n",
    "\n",
    "# Get an example\n",
    "sorted_treebank_trees = sorted(treebank_trees, key=lambda tree: len(tree.leaves()))\n",
    "test_example = sorted_treebank_trees[31].copy()\n",
    "print(test_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1: Describe the above dataset! (1P)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Penn Treebank dataset offers a sentence structure parse tree. It identifies the sentence structure in an easy to understand way. Taking a look at the example, it splits into Subject (NP-SBJ) \"This\", Verb Phrase (VP) \"is\" Predicate (NP-PRD) Japan, and Punctuation. It also labels each word with its respective part of speech. For instance, \"This\" is a determiner (DT). Finally, the data set presents a tree-like structure that demonstrates the combinations of each component to create a sentence.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your goal is to generate syntactic tree structures for English sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2: Describe how you can develop a training approach for a bidirectonal RNN in order to perform Syntactic Constituency Parsing using a sequence-to-sequence approach on the above dataset. (2P)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bidirectional RNN for syntactic component analysis must implement a decoder network that reads the input sequence and processes it in both directions to capture contextual information and generate a parsed tree structure. \n",
    " First, we need to perform the equivalent of data-mapping. An input sequence representing a sentence in the Penn Treebank dataset is converted to a numeric representation by tokenizing the sentence as a word, and the output sequence representing the tree structure for that sentence is encoded as a sequence of numbers.\n",
    "An effective model is one that minimizes the difference between the predicted tree structure and the actual tree structure. Therefore, using a loss function such as the cross-entropy loss function, the weights should be adjusted to minimize the loss.\n",
    "After that, the dataset should be validated, and the accuracy of predicting the tree structure should be used as an evaluation factor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3: What issues do you think can arrise from a sequence to sequence approach for this task? (1P)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Sequence-to-sequence models work best when input and output pairs are of fixed length, so additional techniques will be needed to handle variable-length sentences. Also, because they process input sequences sequentially, they lack context about the overall sentence structure, which may not be favorable for capturing long-term dependencies. In addition, the hierarchical nature of tree structures means that a simple sequence-to-sequnce approach may not be effective.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "An easier way to solve this task is to transform the task to a sequence labeling task.\n",
    "\n",
    "**Task 4: Explain what a sequence labeling task is! (2P)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sequence labeling task aims to predict a label for each element of a sequence by assigning a sequence of labels to each sequence element. An example is POS tagging, which assigns a part of speech or lexical class to each word in a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To transform the task to a sequence labeling task, we do the following:\n",
    "\n",
    "Let $w_i$ be a word located at position $i$ in the sentence, for $$1 \\leq i \\leq \\vert w \\vert −1.$$ We will assign it a 2-tuple label $$l_i = (n_i , c_i ),$$where: $n_i$ is an integer that encodes the number of common ancestors between $w_i$ and $w_{i+1}$, and $c_i$ is the nonterminal symbol at the lowest common ancestor.\n",
    "\n",
    "Encoding:\n",
    "\n",
    "1. **Absolute scale**: The simplest encoding is to make $n_i$ directly equal to the number of ancestors in common between $w_i$ and $w_{i+1}$.\n",
    "2. **Relative scale**: A second and better variant consists in making $n_i$ represent the difference with respect to the number of ancestors encoded in $n_{i−1}$. Its main advantage is that the size of the label set is reduced considerably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**\n",
    "\n",
    "<img src=\"05_example.png\" alt=\"drawing\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5: Write a linearized tree in an relative scale encoding for the following input. (2P)**\n",
    "\n",
    "(S (NP-SBJ (DT This)) (VP (VBZ is) (NP-PRD (NNP Japan))) (. ?))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP-SBJ (DT This)) (VP (VBZ is) (NP-PRD (NNP Japan))) (. ?))\n"
     ]
    }
   ],
   "source": [
    "print(test_example)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer is **\"1S  1VP  -1S\"**.\n",
    "\n",
    "We can specify each word like this.\n",
    "\n",
    "\n",
    "$w_1$ : \"This\"\n",
    "$w_2$ : \"is\"\n",
    "$w_3$ : \"Japan\"\n",
    "$w_4$ : \"?\"\n",
    "\n",
    "\n",
    "First,\n",
    "1. There is only one common ancestor betweem $w_1$ and $w_2$, and the nonterminal symbal at the lowest common ancestor is **S**.\n",
    "2. There is two common ancestor betweem $w_2$ and $w_3$, and the nonterminal symbal at the lowest common ancestor is **VP**.\n",
    "3. There is only one common ancestor betweem $w_3$ and $w_4$, and the nonterminal symbal at the lowest common ancestor is **S**.\n",
    "\n",
    "\n",
    "A linearized tree in absolute scale, **\"1S  2VP  1S\"**.\n",
    "\n",
    "\n",
    "Therefore, a linearized tree in relative scale, **\"1S  1VP  -1S\"**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6: Now write a function that converts the tree to a linearized tree in an relative scale encoding (2P)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your answer here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tree:\n",
      "(S (NP-SBJ (DT This)) (VP (VBZ is) (NP-PRD (NNP Japan))) (. ?))\n",
      "\n",
      "Transformed Tree:\n",
      "  (w1 (1, S))\n",
      "  (w2 (1, VP))\n",
      "  (w3 (-1, S))\n"
     ]
    }
   ],
   "source": [
    "# Function to get the common ancestors and their count using the relative scale\n",
    "def relative_scale_encoding(tree):\n",
    "    encoding = []  # 결과를 저장할 리스트 초기화\n",
    "    leaves = tree.leaves()  # 트리의 잎 노드(단어) 가져오기\n",
    "   \n",
    "    temp = 0\n",
    "    for i in range(len(leaves) - 1):\n",
    "        w_i = leaves[i]   # 현재 단어\n",
    "        w_i_1 = leaves[i + 1]  # 다음 단어\n",
    "        \n",
    "        common_ancestor = tree.treeposition_spanning_leaves(i, i + 2)  # 현재 단어와 다음 단어의 최소 공통 조상 가져오기\n",
    "        common_ancestor_count = len(common_ancestor) + 1  # 최소 공통 조상의 깊이 계산\n",
    "\n",
    "\n",
    "        # 상대적 척도 계산\n",
    "        relative_scale = common_ancestor_count if i == 0 else common_ancestor_count - temp\n",
    "\n",
    "\n",
    "        # 결과 리스트에 상대적 척도와 레이블 추가\n",
    "        label = (relative_scale, tree[common_ancestor].label())\n",
    "        encoding.append(label)\n",
    "\n",
    "\n",
    "        temp = common_ancestor_count\n",
    "   \n",
    "    return encoding\n",
    "\n",
    "# Get the common ancestors and their count for each pair of adjacent words\n",
    "encoding = relative_scale_encoding(test_example)\n",
    "\n",
    "\n",
    "# Print the transformed tree in the specified format\n",
    "print(\"Original Tree:\")\n",
    "print(test_example)\n",
    "print(\"\\nTransformed Tree:\")\n",
    "for i, (n_i, c_i) in enumerate(encoding, start=1):\n",
    "   print(f\"  (w{i} ({n_i}, {c_i}))\")\n",
    "\n",
    "\n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can train on our dataset, we have to transform the labels to class indices. For this purpose, we will use the LabelEncoder from sklearn (https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)\n",
    "\n",
    "For each tree, the last token gets a dummy variable (e.g. [\"-1\", \"DUMMY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\anaconda3\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 7: Create Class Labels. Fill in sentences, encodings and labels variables. (2P)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentences: [['This', 'is', 'Japan', '?']]\n",
      "Encodings: [[1, 'S'], [1, 'VP'], [-1, 'S'], [-1, 'DUMMY']]\n",
      "Class Labels: [2 3 1 0]\n"
     ]
    }
   ],
   "source": [
    "# You might have to install sklearn first. Use the following command and execute it in another cell:\n",
    "# !pip install -U scikit-learn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "'''\n",
    "Your Code here.\n",
    "Transform the label pairs (e.g. (1, S)) to class labels. Fill in sentences, encodings and labels\n",
    "'''\n",
    "\n",
    "sentences = []\n",
    "# sentences = Holds the sentence of the tree\n",
    "\n",
    "encodings = []\n",
    "# encodings = Holds the label pairs with a dummy variable at the end\n",
    "# Hint: I would encode the labels with the dummy variable as ['-1', 'DUMMY']\n",
    "# For our test example: [['1', 'S'], ['1', 'VP'], ['-1', 'S'], ['-1', 'DUMMY']]\n",
    "\n",
    "class_labels = []\n",
    "# labels = Holds the class indices, e.g. [768, 829, 165, 72] for one sentence (each token has one)\n",
    "# Hint: I would transform each pair to a string by joining them with \"_\"\n",
    "# E.g. '1_S', '1_VP', '-1_S', '-1_DUMMY', ...\n",
    "# Collect all labels and fit your LabelEncoder on all labels (use .fit() method)\n",
    "# Then transform each example with your LabelEncoder (use .transform() method)\n",
    "# For our Test Example, I got: [768 829 165 72]\n",
    "\n",
    "# 레이블 쌍을 클래스 레이블로 변환\n",
    "# 예를 들어, (1, S)를 '1_S'로 변환\n",
    "# 마지막으로 더미 레이블 '-1_DUMMY'를 추가\n",
    "labels = [f\"{n}{'' if n == 0 else '_'}{c}\" for n, c in encoding] + ['-1_DUMMY']\n",
    "\n",
    "# LabelEncoder를 초기화하고 모든 레이블에 적합화\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(labels)\n",
    "\n",
    "# LabelEncoder를 사용하여 각 예제를 변환\n",
    "class_labels = label_encoder.transform(labels)\n",
    "\n",
    "# 결과를 출력\n",
    "# sentences, encodings, labels 변수를 채움\n",
    "sentences = [test_example.leaves()]  # 트리의 단어들을 가져와서 sentences 리스트에 추가\n",
    "encodings = [[n, 'DUMMY'] if c == 'DUMMY' else [n, c] for n, c in encoding] + [[-1, 'DUMMY']]  # 레이블 쌍에 더미 레이블을 추가\n",
    "class_labels_encoded = label_encoder.transform([f\"{n}_{c}\" for n, c in encoding] + [\"-1_DUMMY\"])  # 클래스 레이블에 더미 레이블을 추가\n",
    "\n",
    "# 결과를 출력합니다.\n",
    "print(\"\\nSentences:\", sentences)    \n",
    "print(\"Encodings:\", encodings)\n",
    "print(\"Class Labels:\", class_labels_encoded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# This were my values. \u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# If you got different ones by using different methods, ignore this assert\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sentences[\u001b[38;5;241m439\u001b[39m] \u001b[38;5;241m==\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJapan\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m encodings[\u001b[38;5;241m439\u001b[39m] \u001b[38;5;241m==\u001b[39m [[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m], [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVP\u001b[39m\u001b[38;5;124m'\u001b[39m], [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m], [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDUMMY\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(class_labels[\u001b[38;5;241m439\u001b[39m], np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m768\u001b[39m, \u001b[38;5;241m829\u001b[39m, \u001b[38;5;241m165\u001b[39m, \u001b[38;5;241m72\u001b[39m]))\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# This were my values. \n",
    "# If you got different ones by using different methods, ignore this assert\n",
    "assert sentences[439] == ['This', 'is', 'Japan', '?']\n",
    "assert encodings[439] == [['1', 'S'], ['1', 'VP'], ['-1', 'S'], ['-1', 'DUMMY']]\n",
    "assert np.array_equal(class_labels[439], np.array([768, 829, 165, 72]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the GloVe embeddings to subsequently use them for initializing our RNN. Use the same GloVe embeddings as we used in Homework 03. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load GloVe embeddings from the text file\n",
    "def load_glove_embeddings(file_path):\n",
    "    embeddings = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            vector = np.array([float(val) for val in values[1:]])\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "glove_file_path = 'glove.6B.50d.txt' \n",
    "glove_embeddings = load_glove_embeddings(glove_file_path)\n",
    "\n",
    "# Create Embedding Index\n",
    "word_to_index = {word: idx + 1 for idx, word in enumerate(glove_embeddings.keys())}\n",
    "index_to_word = {idx + 1: word for idx, word in enumerate(glove_embeddings.keys())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 8: Build a PyTorch Dataset where the input consists of GloVe word indices, and utilize the class indices generated in the previous task as labels. (2P)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 53\u001b[0m\n\u001b[0;32m     51\u001b[0m dev_labels \u001b[38;5;241m=\u001b[39m class_labels[\u001b[38;5;241m3000\u001b[39m:\u001b[38;5;241m3139\u001b[39m]\n\u001b[0;32m     52\u001b[0m dev_tree_dataset \u001b[38;5;241m=\u001b[39m TreeDatasetGloVeIndexed(dev_sentences, dev_labels)\n\u001b[1;32m---> 53\u001b[0m dev_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dev_tree_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Test dataloader\u001b[39;00m\n\u001b[0;32m     59\u001b[0m test_sentences \u001b[38;5;241m=\u001b[39m sentences[\u001b[38;5;241m3139\u001b[39m:]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:349\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[1;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[1;32m--> 349\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m RandomSampler(dataset, generator\u001b[38;5;241m=\u001b[39mgenerator)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    351\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\sampler.py:140\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[1;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "Your code here.\n",
    "\n",
    "\n",
    "Create PyTorch Dataset, where the input is the sentence indices of GloVe and\n",
    "the created labels as class indices in the previous task. The dataset should be\n",
    "created such that you can train an RNN later on the sentences (with GloVe vocab) and predicts the\n",
    "class indices.\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "class TreeDatasetGloVeIndexed(Dataset):\n",
    "    def __init__(self, sentences, class_labels):\n",
    "      self.sentences = sentences\n",
    "      self.class_labels = class_labels\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_sentence = self.sentences[idx]\n",
    "        label = self.class_labels[idx]\n",
    "        return torch.tensor(input_sentence), torch.tensor(label)\n",
    "\n",
    "\n",
    "# We use a batch size of 1 only.\n",
    "batch_size = 1\n",
    "\n",
    "\n",
    "# Train dataloader\n",
    "train_sentences = sentences[:3000]\n",
    "train_labels = class_labels[:3000]\n",
    "train_tree_dataset = TreeDatasetGloVeIndexed(train_sentences, train_labels)\n",
    "train_dataloader = DataLoader(train_tree_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Dev/Val dataloader\n",
    "dev_sentences = sentences[3000:3139]\n",
    "dev_labels = class_labels[3000:3139]\n",
    "dev_tree_dataset = TreeDatasetGloVeIndexed(dev_sentences, dev_labels)\n",
    "dev_dataloader = DataLoader(dev_tree_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Test dataloader\n",
    "test_sentences = sentences[3139:]\n",
    "test_labels = class_labels[3139:]\n",
    "test_tree_dataset = TreeDatasetGloVeIndexed(test_sentences, test_labels)\n",
    "test_dataloader = DataLoader(test_tree_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 9: Create a bidirectional RNN that initialized the embedding with the GloVe embedding! (2P)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "Your Code here.\n",
    "\n",
    "\n",
    "Create a bidirectional RNN that initialized the embedding with the GloVe embedding!\n",
    "\n",
    "\n",
    "Hint: Use nn.RNN() to create your RNN - Dont have to implement it yourself!\n",
    "\n",
    "\n",
    "'''\n",
    "# Define the bidirectional RNN model with GloVe embeddings initialization\n",
    "class TreeBiRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, glove_embeddings):\n",
    "        super(TreeBiRNN, self).__init__()\n",
    "       \n",
    "        self.embedding = nn.Embedding.from_pretrained(glove_embeddings, freeze = True)\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, _ = self.rnn(embedded)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# Define parameters\n",
    "input_size = 50  # GloVe embeddings size\n",
    "hidden_size = 64\n",
    "output_size = 2\n",
    "\n",
    "\n",
    "# Create the RNN model, loss function, and optimizer\n",
    "model = TreeBiRNN(input_size, hidden_size, output_size, glove_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 10: Now create a training loop and train on your created dataloader! After 100 steps: Collect the training loss and the accuracy on the val/dev dataset . Plot the loss and accuracy curve at the end. (3P)**\n",
    "\n",
    "_Hint: Similar to Homework 03._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Set Hyperparameters\n",
    "num_epochs = 1 # You can change the epoch to get better performance\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Your Variables to fill!\n",
    "collect_loss_after_steps = 100 \n",
    "collected_losses = []\n",
    "collected_val_accuarcy = []\n",
    "\n",
    "'''\n",
    "Your Code Here. This might take some minutes to run!\n",
    "\n",
    "Create a training loop and train on your created dataloader!\n",
    "After 100 steps: Collect the training loss and the accuracy on the val/dev dataset. \n",
    "Plot the loss and accuracy curve at the end. \n",
    "\n",
    "Feel free to test different RNN settings out to get the best performance! \n",
    "If one epoch is running too long, you can reduce the number of steps for training.\n",
    "\n",
    "Hint: You have to transform the labels so that the dimensions are (batch_size x sequence_length x n_classes),\n",
    "where the labels are one hot vectors: labels = F.one_hot(labels, num_classes=output_size).float()\n",
    "\n",
    "'''\n",
    "\n",
    "# train_dataloader holds the test data\n",
    "# dev_dataloader holds the dev/val data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 11: Now, test your bidirectional RNN! Use accuracy as a metric. (1P)**\n",
    "\n",
    "_Hint: You can use the same function that you already implemented to calculate the accuracy for the dev/val set_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing loop\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "'''\n",
    "Your Code Here.\n",
    "\n",
    "Test your bidirectional RNN! Use accuracy as a metric.\n",
    "\n",
    "'''\n",
    "\n",
    "# test_dataloader holds the test data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
