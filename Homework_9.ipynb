{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Homework 9: Transformer Models\n",
    "#### Introduction to Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hyerin, Seo. (hyseo@students.uni-mainz.de)\n",
    "* Yeonwoo, Nam. (yeonam@students.uni-mainz.de)\n",
    "* Yevin, Kim. (kyevin@students.uni-mainz.de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can reach 20 points on this homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this assignment, we'll be using transformer models to tackle the inflection task from Homework 06. We'll make use of the widely used Hugging Face library (https://huggingface.co). Don't worry, this assignment serves as a simple introduction to this new framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have questions, you can reach out via mail: minhducbui@uni-mainz.de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Downloading Huggingface and Playing around\n",
    "\n",
    "In this section, we'll first download huggingface and get familiar with the framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following cell or in your terminal, if this doesnt work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\user\\anaconda3\\lib\\site-packages (4.36.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in c:\\users\\user\\anaconda3\\lib\\site-packages (0.25.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\user\\anaconda3\\lib\\site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from accelerate) (2.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from accelerate) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from accelerate) (1.21.5)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\user\\anaconda3\\lib\\site-packages (from accelerate) (0.19.4)\n",
      "Requirement already satisfied: psutil in c:\\users\\user\\anaconda3\\lib\\site-packages (from accelerate) (5.8.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from accelerate) (0.4.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from packaging>=20.0->accelerate) (3.0.4)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (2.11.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.6.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.10.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (4.1.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface-hub->accelerate) (4.64.0)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface-hub->accelerate) (2.27.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub->accelerate) (0.4.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->accelerate) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2021.10.8)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\user\\anaconda3\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (1.4.2)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (2023.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.18.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (0.19.4)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (14.0.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (4.64.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (1.21.5)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (2.27.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (5.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.6.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from async-timeout<5.0,>=4.0.0a3->aiohttp->datasets) (4.1.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.18.0->datasets) (3.6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from packaging->datasets) (3.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\user\\anaconda3\\lib\\site-packages (0.1.99)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have to restart your kernel after installing everything!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5 Model Playing Around"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task, we are using the T5 Model: https://arxiv.org/abs/1910.10683\n",
    "\n",
    "Before we start with the task, we have to download the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1: Describe the pre-training task of T5! (2P)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T5 stands for \"Text-to-Text Transfer Transformer,\" and it is focused on addressing natural language processing problems by framing them as \"Text-to-Text\" problems. This involves training the model to handle all natural language processing (NLP) tasks by transforming them into a text input to text output format.\n",
    "\n",
    "The pre-training process of T5 utilizes the corruption masking technique. In this approach, a portion of the input text is randomly masked, and the model is trained to predict and restore the masked portion. Essentially, T5 is trained to predict and reconstruct missing parts within a given text, enhancing its ability to understand context and integrate relevant information. Through this process, the model improves its proficiency in comprehending context and restoring missing information within the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2: Explain the following Code Snippet! Explain all steps. (2P)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "studies have shown that owning a dog is good for you.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "# training\n",
    "input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n",
    "labels = tokenizer(\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\", return_tensors=\"pt\").input_ids\n",
    "outputs = model(input_ids=input_ids, labels=labels)\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits\n",
    "\n",
    "# inference\n",
    "input_ids = tokenizer(\n",
    "    \"summarize: studies have shown that owning a dog is good for you\", return_tensors=\"pt\"\n",
    ").input_ids  # Batch size 1\n",
    "outputs = model.generate(input_ids, max_new_tokens=50)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "Using the AutoTokenizer with the configuration \"t5-small\", the tokenizer for the T5 model is initialized then loaded. The tokenizer converts text into a format that can be input into the model and is automatically selected based on the specified model architecture.\n",
    "\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "An instance of the T5ForConditionalGeneration model is created, and the pre-trained weights from the \"t5-small\" checkpoint are loaded to initialize the T5 model for conditional text generation. This class is specialized for conditional text generation tasks.\n",
    "\n",
    "input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n",
    "labels = tokenizer(\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\", return_tensors=\"pt\").input_ids\n",
    "The input text and labels are tokenized using the tokenizer. The input sequence contains placeholders like <extra_id_0> and <extra_id_1>, which are special tokens that will be filled with specific content during training. The argument return_tensors=\"pt\" indicates that the returned values should be PyTorch tensors.\n",
    "\n",
    "\n",
    "outputs = model(input_ids=input_ids, labels=labels)\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits\n",
    "The input tokens (input_ids) and target labels (labels) are passed to the T5 model. The model calculates the loss and logits (raw output scores). It is for minimizing this loss by adjusting the model parameters to generate outputs close to the target labels.\n",
    "\n",
    "\n",
    "input_ids = tokenizer(\"summarize: studies have shown that owning a dog is good for you\", return_tensors=\"pt\").input_ids\n",
    "outputs = model.generate(input_ids, max_new_tokens=50)\n",
    "For inference, a new input sequence with a summarization prompt is prepared. Inference involves tokenizing the new input sequence and using the generate method to generate text. The max_new_tokens parameter specifies the maximum number of tokens the generated output can have.\n",
    "\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "The generated output tokens are decoded to a human-readable string, and skip_special_tokens=True removes special tokens like <extra_id_0> from the final output. The decoded text is printed, providing the summary based on the provided input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Training T5!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a T5 model for the inflection task in Homework 06. You'll notice that training is significantly simplified with this framework.\n",
    "\n",
    "Load the dataset from Homework 06!\n",
    "\n",
    "**IMPORTANT:** This assignment involves computationally intensive tasks. We strongly recommend testing your code on a small subset first. You may also submit the homework with a minimal amount of data if needed. For that, just change take a subset of the training/test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:\n",
      "Number of training samples: 1000\n",
      "Example sentences:\n",
      "   ['Reflektion', 'Reflektionen', 'N;ACC;PL']\n",
      "   ['Scherz', 'Scherzes', 'N;GEN;SG']\n",
      "\n",
      "Dev Data:\n",
      "Number of development samples: 50\n",
      "\n",
      "Test Data:\n",
      "Number of test samples: 1000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_dir = \"morphological\"\n",
    "\n",
    "# Define the file paths\n",
    "train_file = os.path.join(data_dir, \"german-train-medium.txt\")\n",
    "dev_file = os.path.join(data_dir, \"german-dev.txt\")\n",
    "test_file = os.path.join(data_dir, \"german-uncovered-test.txt\")\n",
    "\n",
    "def read_conll_file(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        current_sentence = []\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if not line:  # Empty line indicates the end of a sentence\n",
    "                if current_sentence:\n",
    "                    data.append(current_sentence)\n",
    "                    current_sentence = []\n",
    "            else:\n",
    "                columns = line.split('\\t')\n",
    "                current_sentence.append(columns)\n",
    "        data += current_sentence\n",
    "    return data\n",
    "\n",
    "# Read data\n",
    "train_data = read_conll_file(train_file)\n",
    "dev_data = read_conll_file(dev_file)\n",
    "# We are going to reduce the amount of dev data\n",
    "dev_data = dev_data[:50]\n",
    "test_data = read_conll_file(test_file)\n",
    "\n",
    "print(\"Train Data:\")\n",
    "print(f\"Number of training samples: {len(train_data)}\")\n",
    "print(f\"Example sentences:\")\n",
    "for example in train_data[-2:]:  # Displaying the last two sentences for illustration\n",
    "    print(f\"   {example}\")\n",
    "\n",
    "print(\"\\nDev Data:\")\n",
    "print(f\"Number of development samples: {len(dev_data)}\")\n",
    "\n",
    "print(\"\\nTest Data:\")\n",
    "print(f\"Number of test samples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download T5 Sequence to Sequence model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from datasets import load_dataset\n",
    "\n",
    "model_name = \"t5-small\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, max_length=64)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, n_positions=64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "**Task 3: Explain the following Code. What are input_ids and attention_mask? How can the attention_mask be useful? (2P)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[30926,   445,   117,  4296,   382,   117,  9945,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[30926,     1]]), 'attention_mask': tensor([[1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "example = train_data[0]\n",
    "input_tokenized = tokenizer(example[0] + \" \" + example[2], return_tensors='pt')\n",
    "print(input_tokenized)\n",
    "\n",
    "output_tokenized = tokenizer(example[1], return_tensors='pt')\n",
    "print(output_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example = train_data[0]\n",
    "It assumes that train_data is a dataset and get the first example from this dataset.\n",
    "\n",
    "\n",
    "input_tokenized = tokenizer(example[0] + \" \" + example[2], return_tensors='pt')\n",
    "print(input_tokenized)\n",
    "The variable `input_tokenized` holds the tokenized result for the input. The input text is combined with additional information, and this concatenated text is tokenized using the tokenizer. For each example mentioned above, `example[0]` corresponds to the input text, `example[1]` to the target (output) text, and `example[2]` to additional information concatenated to the input text. The argument `return_tensors='pt'` indicates that the returned values should be PyTorch tensors. The printed output includes the tokenized representation of the input text, containing `input_ids` and `attention_mask`.\n",
    "\n",
    "output_tokenized = tokenizer(example[1], return_tensors='pt')\n",
    "print(output_tokenized)\n",
    "The variable `output_tokenized` stores the tokenized result for the target (output) text. Similarly, the printed output includes the tokenized representation of the output text, containing `input_ids` and `attention_mask`.\n",
    "\n",
    "input_ids\n",
    "It is a sequence of token IDs representing the tokenized version of the input text. Each token in the input text is mapped to a unique numerical ID in the model's vocabulary. This tensor is what the model will use as input during training or inference.\n",
    "\n",
    "attention_mask\n",
    "It is a binary tensor that informs the model about which tokens are padding and which ones are actual input data, indicating their importance. The `attention_mask` has the same shape as `input_ids`, with 1 filled at positions where the corresponding token in `input_ids` requires attention as an actual token and 0 filled where padding tokens should be ignored.\n",
    "\n",
    "Reason why attention_mask is useful\n",
    "The `attention_mask` indicates which parts the model should pay attention to. It sets the padded parts to 0, allowing the model to ignore them and focus only on the actual input. This enhances computational performance, especially when dealing with input sequences of varying lengths.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4: Write a PyTorch dataset for our task. (3P)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tokenized_data, tokenizer):\n",
    "        # 토크나이저 및 토큰화된 데이터 초기화\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenized_data = tokenized_data\n",
    "\n",
    "    def __len__(self):\n",
    "        # 데이터셋의 전체 길이를 반환\n",
    "        return len(self.tokenized_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 인덱스에 해당하는 데이터 추출\n",
    "        input_text = self.tokenized_data[idx][0] + \" \" + self.tokenized_data[idx][2]\n",
    "        output_text = self.tokenized_data[idx][1]\n",
    "\n",
    "        # 입력 및 출력 텍스트를 토크나이저로 처리하여 텐서로 변환\n",
    "        input_tokenized = self.tokenizer(input_text, return_tensors='pt', padding=True, truncation=True)\n",
    "        output_tokenized = self.tokenizer(output_text, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "        # PyTorch 데이터로 반환\n",
    "        return {\n",
    "            \"input_ids\": input_tokenized['input_ids'].squeeze(),\n",
    "            \"labels\": output_tokenized['input_ids'].squeeze()\n",
    "        }\n",
    "\n",
    "\n",
    "# PyTorch 데이터셋 생성\n",
    "train_dataset = CustomDataset(train_data, tokenizer)\n",
    "dev_dataset = CustomDataset(dev_data, tokenizer)\n",
    "test_dataset = CustomDataset(test_data, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.equal(dev_dataset[0][\"input_ids\"], torch.tensor([26801,   445,   117, 14775,   117,  5329,     1]))\n",
    "assert torch.equal(dev_dataset[0][\"labels\"], torch.tensor([26801,    35,     1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5: Explain the questions inside the relevant code snippets. (3P)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "\n",
    "# (1) What is this function calculating?\n",
    "def compute_metrics(preds):\n",
    "    output, labels = preds\n",
    "    logits = output[0]\n",
    "    logits = torch.tensor(logits)\n",
    "    labels = torch.tensor(labels)\n",
    "    # (2) What is this step doing?\n",
    "    predictions = logits.argmax(-1)\n",
    "\n",
    "    # (3) Why do you think we had -100 inside our labels? \n",
    "    # This is done by the framework.\n",
    "    labels = torch.where(labels != -100, labels, torch.tensor(0))\n",
    "\n",
    "    correct_sequences = torch.all(predictions == labels, dim=1)\n",
    "    accuracy = torch.mean(correct_sequences.float())\n",
    "    return {\"accuracy\": accuracy.item()}\n",
    "\n",
    "\n",
    "# (4) What is the trainer replacing compared to our normal PyTorch model training? \n",
    "# Name atleast three components\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,  # Number of steps before evaluation\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) This function calculates accuracy by comparing model predictions with actual ground truth labels. In simple terms, it extracts model predictions and true labels from 'output, labels = preds.' Then, it obtains the logits for each class from the output, assigns the class with the highest logit value to 'predictions.' Subsequently, the code 'correct_sequences = torch.all(predictions == labels, dim=1)' checks if the predicted sequence matches the actual sequence. Finally, it computes the average accuracy.\n",
    "\n",
    "(2) The step 'predictions = logits.argmax(-1)' selects the class with the highest logit value. In other words, it plays the role of finding the class with the maximum predicted probability for the input sequence.\n",
    "\n",
    "(3) The value -100 represents the absence of labels. Specifically, -100 is used to signify tokens, such as padding tokens, that should not contribute to the loss calculation. The model does not calculate loss at positions with -100, ensuring that these positions do not influence the model's training.\n",
    "\n",
    "(4) Using Seq2SeqTrainer introduces differences compared to normal PyTorch model training. Firstly, the trainer automatically handles data loading, unlike in normal PyTorch training where users have to define it explicitly. Secondly, the trainer internally manages the training loop, handling gradient descent and model parameter updates automatically, in contrast to the user-defined training loop in normal PyTorch model training. Lastly, while in normal PyTorch training users have to implement metric calculations directly, using Seq2SeqTrainer allows the trainer to automatically compute metrics when provided a function through 'compute_metrics.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 10:09, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.420600</td>\n",
       "      <td>1.747426</td>\n",
       "      <td>0.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.082100</td>\n",
       "      <td>1.545658</td>\n",
       "      <td>0.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.504000</td>\n",
       "      <td>1.484275</td>\n",
       "      <td>0.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.828000</td>\n",
       "      <td>1.415549</td>\n",
       "      <td>0.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.596900</td>\n",
       "      <td>1.397326</td>\n",
       "      <td>0.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.405100</td>\n",
       "      <td>1.341447</td>\n",
       "      <td>0.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.705700</td>\n",
       "      <td>1.319453</td>\n",
       "      <td>0.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.984800</td>\n",
       "      <td>1.238837</td>\n",
       "      <td>0.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.533600</td>\n",
       "      <td>1.233170</td>\n",
       "      <td>0.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.647400</td>\n",
       "      <td>1.225518</td>\n",
       "      <td>0.320000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./output\\checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./output\\checkpoint-1000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=1.7708260498046875, metrics={'train_runtime': 609.9735, 'train_samples_per_second': 1.639, 'train_steps_per_second': 1.639, 'total_flos': 3212253069312.0, 'train_loss': 1.7708260498046875, 'epoch': 1.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute the training here! This can take a while, scale down the data if necessarily.\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6: Now write your own evaluation (accuracy) on the test set using PyTorch style. (3P)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10/1000 examples - Correct: 5.169047713279724\n",
      "Processed 20/1000 examples - Correct: 12.096825487911701\n",
      "Processed 30/1000 examples - Correct: 20.203968413174152\n",
      "Processed 40/1000 examples - Correct: 26.192857317626476\n",
      "Processed 50/1000 examples - Correct: 33.492857329547405\n",
      "Processed 60/1000 examples - Correct: 39.39047644287348\n",
      "Processed 70/1000 examples - Correct: 46.434920974075794\n",
      "Processed 80/1000 examples - Correct: 52.98015909641981\n",
      "Processed 90/1000 examples - Correct: 60.33015915006399\n",
      "Processed 100/1000 examples - Correct: 68.95873060077429\n",
      "Processed 110/1000 examples - Correct: 77.44920682162046\n",
      "Processed 120/1000 examples - Correct: 86.34920685738325\n",
      "Processed 130/1000 examples - Correct: 94.26587354391813\n",
      "Processed 140/1000 examples - Correct: 101.32301642745733\n",
      "Processed 150/1000 examples - Correct: 108.48968317359686\n",
      "Processed 160/1000 examples - Correct: 117.08968322724104\n",
      "Processed 170/1000 examples - Correct: 123.8896832242608\n",
      "Processed 180/1000 examples - Correct: 131.07301657646894\n",
      "Processed 190/1000 examples - Correct: 136.4896833077073\n",
      "Processed 200/1000 examples - Correct: 144.47301668673754\n",
      "Processed 210/1000 examples - Correct: 152.02301669865847\n",
      "Processed 220/1000 examples - Correct: 158.68968341499567\n",
      "Processed 230/1000 examples - Correct: 166.10635013133287\n",
      "Processed 240/1000 examples - Correct: 173.9063501432538\n",
      "Processed 250/1000 examples - Correct: 180.8420644775033\n",
      "Processed 260/1000 examples - Correct: 188.1587312296033\n",
      "Processed 270/1000 examples - Correct: 195.67539796978235\n",
      "Processed 280/1000 examples - Correct: 202.04444558173418\n",
      "Processed 290/1000 examples - Correct: 210.6277788951993\n",
      "Processed 300/1000 examples - Correct: 217.52777890115976\n",
      "Processed 310/1000 examples - Correct: 223.8444455936551\n",
      "Processed 320/1000 examples - Correct: 230.42777896672487\n",
      "Processed 330/1000 examples - Correct: 238.01111233979464\n",
      "Processed 340/1000 examples - Correct: 242.67777904123068\n",
      "Processed 350/1000 examples - Correct: 250.47777908295393\n",
      "Processed 360/1000 examples - Correct: 257.9420648291707\n",
      "Processed 370/1000 examples - Correct: 266.29206485301256\n",
      "Processed 380/1000 examples - Correct: 272.38254108279943\n",
      "Processed 390/1000 examples - Correct: 279.40516018122435\n",
      "Processed 400/1000 examples - Correct: 285.8551601842046\n",
      "Processed 410/1000 examples - Correct: 291.9718268737197\n",
      "Processed 420/1000 examples - Correct: 298.26706502586603\n",
      "Processed 430/1000 examples - Correct: 304.03373173624277\n",
      "Processed 440/1000 examples - Correct: 310.77182703465223\n",
      "Processed 450/1000 examples - Correct: 317.45516043156385\n",
      "Processed 460/1000 examples - Correct: 326.6051604375243\n",
      "Processed 470/1000 examples - Correct: 333.30992237478495\n",
      "Processed 480/1000 examples - Correct: 340.65039855986834\n",
      "Processed 490/1000 examples - Correct: 348.2646842971444\n",
      "Processed 500/1000 examples - Correct: 353.7480176910758\n",
      "Processed 510/1000 examples - Correct: 360.2980177626014\n",
      "Processed 520/1000 examples - Correct: 366.35039875656366\n",
      "Processed 530/1000 examples - Correct: 373.65516067296267\n",
      "Processed 540/1000 examples - Correct: 379.72182739526033\n",
      "Processed 550/1000 examples - Correct: 384.98849407583475\n",
      "Processed 560/1000 examples - Correct: 391.4884941354394\n",
      "Processed 570/1000 examples - Correct: 396.3742084726691\n",
      "Processed 580/1000 examples - Correct: 402.85992281883955\n",
      "Processed 590/1000 examples - Correct: 410.47063716501\n",
      "Processed 600/1000 examples - Correct: 418.07063718885183\n",
      "Processed 610/1000 examples - Correct: 426.4515896067023\n",
      "Processed 620/1000 examples - Correct: 434.1718277260661\n",
      "Processed 630/1000 examples - Correct: 439.4599230661988\n",
      "Processed 640/1000 examples - Correct: 447.55992314964533\n",
      "Processed 650/1000 examples - Correct: 454.80278039723635\n",
      "Processed 660/1000 examples - Correct: 462.5908756926656\n",
      "Processed 670/1000 examples - Correct: 468.66230434924364\n",
      "Processed 680/1000 examples - Correct: 473.1789710447192\n",
      "Processed 690/1000 examples - Correct: 479.2956377789378\n",
      "Processed 700/1000 examples - Correct: 484.84563782066107\n",
      "Processed 710/1000 examples - Correct: 491.3289712294936\n",
      "Processed 720/1000 examples - Correct: 498.74563793092966\n",
      "Processed 730/1000 examples - Correct: 506.3456379547715\n",
      "Processed 740/1000 examples - Correct: 513.9956379905343\n",
      "Processed 750/1000 examples - Correct: 520.995638050139\n",
      "Processed 760/1000 examples - Correct: 528.6956380978227\n",
      "Processed 770/1000 examples - Correct: 534.9813524261117\n",
      "Processed 780/1000 examples - Correct: 542.71944770962\n",
      "Processed 790/1000 examples - Correct: 550.0527810528874\n",
      "Processed 800/1000 examples - Correct: 554.919447787106\n",
      "Processed 810/1000 examples - Correct: 561.4908764436841\n",
      "Processed 820/1000 examples - Correct: 567.6492098346353\n",
      "Processed 830/1000 examples - Correct: 573.8825431540608\n",
      "Processed 840/1000 examples - Correct: 581.1492098644376\n",
      "Processed 850/1000 examples - Correct: 588.6158765628934\n",
      "Processed 860/1000 examples - Correct: 593.5396861508489\n",
      "Processed 870/1000 examples - Correct: 601.4730195179582\n",
      "Processed 880/1000 examples - Correct: 608.0063528493047\n",
      "Processed 890/1000 examples - Correct: 615.2230195477605\n",
      "Processed 900/1000 examples - Correct: 621.5896862074733\n",
      "Processed 910/1000 examples - Correct: 629.6611148491502\n",
      "Processed 920/1000 examples - Correct: 635.1277815774083\n",
      "Processed 930/1000 examples - Correct: 641.2777815982699\n",
      "Processed 940/1000 examples - Correct: 649.3134958967566\n",
      "Processed 950/1000 examples - Correct: 654.4242102131248\n",
      "Processed 960/1000 examples - Correct: 661.3051626160741\n",
      "Processed 970/1000 examples - Correct: 669.9301626160741\n",
      "Processed 980/1000 examples - Correct: 677.8087340965867\n",
      "Processed 990/1000 examples - Correct: 685.1420674696565\n",
      "Processed 1000/1000 examples - Correct: 690.9420675262809\n",
      "Overall Accuracy: 69.09%\n"
     ]
    }
   ],
   "source": [
    "# 배치 크기 및 로깅 간격 설정\n",
    "batch_size = 1\n",
    "log_interval = 10\n",
    "# 테스트 데이터 로더 설정\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# 모델을 평가 모드로 설정 및 초기화\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "\n",
    "# 평가를 위한 루프\n",
    "with torch.no_grad():\n",
    "    for idx, batch in enumerate(test_dataloader):\n",
    "        # 입력 데이터를 지정한 디바이스로 이동\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "\n",
    "        # 훈련된 모델을 사용하여 텍스트 생성\n",
    "        outputs = model.generate(input_ids, max_length=50)  # Adjust max_length as needed\n",
    "\n",
    "        # 생성된 텍스트 디코딩 및 출력\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "         # 생성된 텍스트를 레이블과 길이에 맞게 자르거나 패딩\n",
    "        generated_text_ids = tokenizer.encode(generated_text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=len(batch[\"labels\"][0])).to(device)\n",
    "\n",
    "        # 생성된 텍스트와 레이블을 비교하여 정확도 계산\n",
    "        labels = batch[\"labels\"].to(device).tolist()\n",
    "        correct_sequences = generated_text_ids == torch.tensor(labels, device=device)\n",
    "        accuracy = torch.mean(correct_sequences.float()).item()\n",
    "        total_correct += accuracy\n",
    "\n",
    "        # 일정 간격으로 진행 상황 로깅\n",
    "        if (idx + 1) % log_interval == 0 or (idx + 1) == len(test_dataloader):\n",
    "            print(f\"Processed {idx+1}/{len(test_dataloader)} examples - Correct: {total_correct}\")\n",
    "\n",
    "# 전체 정확도 계산\n",
    "overall_accuracy = total_correct / len(test_dataloader)\n",
    "print(f\"Overall Accuracy: {overall_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 7: Write a function, where I can input a word (as a string) with its inflection features, and it outputs the models prediction as a string! (3P)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def model_prediction_as_word(input_str, inflection_features, model, tokenizer):\n",
    "    # 입력 단어와 변형 특징을 결합\n",
    "    input_text = f\"{input_str} {inflection_features}\"\n",
    "\n",
    "    # 입력 텍스트를 토큰화\n",
    "    input_ids = tokenizer(input_text, return_tensors='pt').input_ids.to(device)\n",
    "\n",
    "    # 훈련된 모델을 사용하여 텍스트 생성\n",
    "    outputs = model.generate(input_ids, max_length=50)  # 필요에 따라 max_length 조절\n",
    "\n",
    "    # 생성된 텍스트 디코딩 및 반환\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 8: Examine the model errors. What is your hypothesis for why the model is not perfect? (2P)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple reason for the model's suboptimal performance could be the insufficiency of training data. Although there were 1000 examples, language, being complex, might require a more substantial dataset for effective learning. Additionally, considering the intricacies of language phenomena, especially with respect to special vocabulary or inflections, it's likely that the tokenizer couldn't comprehend and handle them perfectly. This limitation in understanding intricate linguistic elements could contribute to the lower accuracy observed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
