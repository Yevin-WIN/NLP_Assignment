{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1: Sentiment Analysis with Na√Øve Bayes\n",
    "#### CSCI 3832 Natural Language Processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Lemmas and inflected forms, hyponyms/hypernyms, the distributional hypothesis\n",
    "2. Tokenization, vocabularies, and feature extraction for a Naive Bayes model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Yevin Kim, kimyevin17@gmail.com*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Free Response Questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1: Write down the lemmas of the following inflected forms:**\n",
    "1. walked\n",
    "2. taught\n",
    "3. best\n",
    "4. are\n",
    "5. running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*\n",
    "1. walk\n",
    "2. teach\n",
    "3. good\n",
    "4. be\n",
    "5. run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2: Write down 3 hyponyms of the following words:**\n",
    "1. dog\n",
    "2. food\n",
    "3. profession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*\n",
    "1. Pomeranian, Poodle, German Shepherd\n",
    "2. pizza, pasta, bread\n",
    "3. doctor, teacher, baker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3: In your own words, describe:**\n",
    "1. The distributional hypothesis (see lecture on distributional semantics)\n",
    "2. How is the distributional hypothesis relvant to NLP systems?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*\n",
    ": 1. In distributional semantics, a word's meaning can vary depending on its placement within a sentence. Thus, its meaning is determined by contextual factors rather than independent words. The distributional hypothesis is grounded in this theory, states that words with similar meanings are likely to be used in similar contexts.\n",
    ": 2. The distributional hypothesis is important for NLP systems in facilitating to understand word relationships. For example, by analysing the neighbouring words (context) of a certain word, a distributional representation of the word can be formed as a vector. This process provides computers with an advantage for computation and representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Sentiment Analysis with Naive Bayes\n",
    "\n",
    "In this section, our goal is to classify a set of movie reviews as positive or negative. For our dataset, we'll use the [Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/). To get started, download the dataset from the link, and extract it to where your notebook is. Next, we'll load the data and look at a couple of examples. \n",
    "\n",
    "*Important: for any project which involves creating or training models, you can **only** do your exploratory data analysis on the training set. Looking at the test set in any way can invalidate your results!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive examples: 12500\n",
      "Number of negative examples: 12500\n",
      "\n",
      "\n",
      "Sample positive example: Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\n",
      "\n",
      "\n",
      "Sample negative example: Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy. A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's better than you might think with some good cinematography by future great Vilmos Zsigmond. Future stars Sally Kirkland and Frederic Forrest can be seen briefly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_dir = 'aclImdb/'\n",
    "\n",
    "pos_train_dir = data_dir + 'train/pos/'\n",
    "neg_train_dir = data_dir + 'train/neg/'\n",
    "\n",
    "def read_folder(folder):\n",
    "    examples = []\n",
    "    for fname in os.listdir(folder):\n",
    "        with open(os.path.join(folder, fname), encoding='utf8') as f:\n",
    "            examples.append(f.readline().strip())\n",
    "    return examples\n",
    "\n",
    "pos_examples = read_folder(pos_train_dir)\n",
    "neg_examples = read_folder(neg_train_dir)\n",
    "\n",
    "print('Number of positive examples: {}\\nNumber of negative examples: {}\\n\\n'.format(len(pos_examples), len(neg_examples)))\n",
    "\n",
    "print('Sample positive example: {}\\n\\n'.format(pos_examples[0]))\n",
    "print('Sample negative example: {}'.format(neg_examples[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've loaded the data, let's create our vocabulary. While we want our vocabulary to cover the whole training set, we'll keep them separate to see if there are any words which are frequently found in one or the other class -- these words might be informative features for classification! \n",
    "\n",
    "The simplest way to create a vocabulary is to split on spaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "pos_words = []  # A list of all space separated tokens found across all positive examples. (Contains duplicates)\n",
    "neg_words = []\n",
    "\n",
    "pos_vocab = set()  # A list of *unique* separated tokens found in across all positive examples. (No duplicates)\n",
    "neg_vocab = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample words from positive examples: ['Bromwell', 'High', 'is', 'a', 'cartoon', 'comedy.', 'It', 'ran', 'at', 'the']\n",
      "\n",
      "\n",
      "Sample words from negative examples: ['Story', 'of', 'a', 'man', 'who', 'has', 'unnatural', 'feelings', 'for', 'a']\n",
      "2958696\n",
      "178873\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "Your code here. For each class (positive/negative) find both the list of types and tokens for each class. \n",
    "To separate each example into separate words, split the example on spaces. \n",
    "'''\n",
    "\n",
    "# positive reviews\n",
    "for example in pos_examples:\n",
    "    token = example.split(' ')\n",
    "    pos_words.extend(token)\n",
    "    pos_vocab.update(token)\n",
    "\n",
    "# negative reviews\n",
    "for example in neg_examples:\n",
    "    token = example.split(' ')\n",
    "    neg_words.extend(token)\n",
    "    neg_vocab.update(token)\n",
    "\n",
    "print('Sample words from positive examples: {}\\n\\n'.format(pos_words[:10]))\n",
    "print('Sample words from negative examples: {}'.format(neg_words[:10]))\n",
    "\n",
    "# just for sanity check\n",
    "print(len(pos_words))\n",
    "print(len(pos_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2958696\n",
      "178873\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "\n",
    "print(len(pos_words))\n",
    "print(len(pos_vocab))\n",
    "\n",
    "assert len(pos_words) == 2958696\n",
    "assert len(pos_vocab) == 178873"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets calculate word frequencies for each class. (Hint: use the Python Counter class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "pos_frequencies = [] # A list of tuples of the form (word, count). \n",
    "                 # The list should be sorted in descending order, using the count of each tuple as the key\n",
    "\n",
    "neg_frequencies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 most common words in positive reviews:\n",
      "the: 148413\n",
      "and: 84270\n",
      "a: 79427\n",
      "of: 75341\n",
      "to: 65209\n",
      "is: 55358\n",
      "in: 45794\n",
      "that: 31941\n",
      "I: 30927\n",
      "it: 26987\n",
      "this: 26021\n",
      "/><br: 24617\n",
      "as: 23930\n",
      "with: 22031\n",
      "was: 21308\n",
      "\n",
      "Top 15 most common words in negative reviews:\n",
      "the: 138612\n",
      "a: 75665\n",
      "and: 68381\n",
      "of: 67629\n",
      "to: 67359\n",
      "is: 47870\n",
      "in: 39782\n",
      "I: 35043\n",
      "that: 32615\n",
      "this: 31177\n",
      "it: 27440\n",
      "/><br: 26318\n",
      "was: 25389\n",
      "for: 20197\n",
      "with: 19687\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "''' \n",
    "Your code here. For each class (positive/negative) calculate the frequency of each word and save it in pos_counter\n",
    "and neg_counter.\n",
    "\n",
    "Print the top 15 most common word for each class. \n",
    "\n",
    "'''\n",
    "\n",
    "# Calculate word frequency by using 'Counter'\n",
    "pos_counter = Counter(pos_words)\n",
    "neg_counter = Counter(neg_words)\n",
    "\n",
    "# Sort word frequency in descending order\n",
    "pos_frequencies = pos_counter.most_common(15)\n",
    "neg_frequencies = neg_counter.most_common(15)\n",
    "\n",
    "# top 15 most common words in positive reviews\n",
    "print(\"Top 15 most common words in positive reviews:\")\n",
    "for word, count in pos_frequencies:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# top 15 most common words in negative reviews\n",
    "print(\"\\nTop 15 most common words in negative reviews:\")\n",
    "for word, count in neg_frequencies:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "assert pos_frequencies[0] == ('the', 148413)\n",
    "assert neg_frequencies[0] == ('the', 138612)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the top 15 words for each class we see two problems:\n",
    "\n",
    "1. The words are essentially the same for each class, which doesn't give us any information on how to differentiate them.\n",
    "2. Look at the most frequent tokens. Are there any tokens which aren't words? Any situations where tokens with different surface forms but the same meaning could be repeated (and if so, how might we control for this?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer to 2 here*\n",
    "\n",
    ": Non-word tokens, such as \"/><br\", are among the most frequent in the text data. These artifacts are often a result of HTML or other formatting present. To enhance the analysis, it is suggested that preliminary text data processing is performed to eliminate all non-word tokens and other formatting artifacts. Furthermore, lemmatization or synonym classification can be used to manage similar meaning tokens with different surface forms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of looking at the most frequent words, let's instead look at the most frequent words which explicitly do not appear in the other class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Edie', 82), ('Gundam', 74), ('Antwone', 58), ('/>8/10', 47), ('/>7/10', 46), ('/>10/10', 45), ('Gunga', 44), ('Gypo', 44), ('Din', 43), ('Othello', 41), ('7/10.', 37), ('Blunt', 37), ('Yokai', 37), ('Tsui', 35), ('Blandings', 34), ('Goldsworthy', 32), ('/>9/10', 31), ('Gino', 31), ('Visconti', 30), ('Bernsen', 29), ('Taker', 29), ('Brashear', 29), ('Harilal', 29), ('Clutter', 28), (\"Goldsworthy's\", 27), ('\"Rob', 26), ('Dominick', 25), ('MJ', 25), ('/>7', 24), ('Rosenstrasse', 24), ('Sassy', 24), ('Flavia', 24), ('Ashraf', 23), ('Recommended.', 22), ('Brock', 22), ('vulnerability', 22), ('Sabu', 22), ('Korda', 22), ('Ahmad', 22), ('Stevenson', 22), ('Coop', 22), ('Riff', 22), ('flawless.', 21), ('aunts', 21), (\"Gilliam's\", 21), ('Solo', 21), ('Kells', 21), (\"Capote's\", 21), ('Cutter', 21), ('Blackie', 21)]\n",
      "\n",
      "\n",
      "[('/>4/10', 56), ('/>Avoid', 55), ('2/10', 49), ('*1/2', 45), ('unwatchable.', 43), ('/>3/10', 40), ('Thunderbirds', 40), ('Gamera', 39), ('steaming', 35), ('Wayans', 33), ('Slater', 31), ('drivel.', 30), ('Tashan', 29), ('Aztec', 29), ('/>1/10', 28), ('Sarne', 27), ('Kareena', 26), ('BTK', 26), ('Segal', 26), ('blah,', 26), ('Delia', 26), ('0/10', 25), ('neither.', 25), ('Gram', 25), ('(*1/2)', 24), ('croc', 24), ('Dahmer', 24), ('Darkman', 24), ('Rosanna', 23), ('Zenia', 23), ('tripe.', 22), ('awful!', 22), ('2/10.', 22), ('Kornbluth', 22), ('Saif', 21), ('incoherent,', 21), ('appallingly', 21), ('Shaq', 21), ('Welch', 21), ('Hackenstein', 21), ('/>2/10', 20), ('4/10.', 20), ('kibbutz', 20), ('Clay', 20), ('Morgana', 20), ('\"1\"', 19), ('crawling', 19), ('/>1', 19), ('awfulness', 19), ('Mraovich', 19)]\n"
     ]
    }
   ],
   "source": [
    "only_pos_words = [word for word in pos_words if word not in neg_vocab]\n",
    "only_neg_words = [word for word in neg_words if word not in pos_vocab]\n",
    "\n",
    "opw_counter = Counter(only_pos_words)\n",
    "onw_counter = Counter(only_neg_words)\n",
    "\n",
    "print(opw_counter.most_common()[:50])\n",
    "print('\\n')\n",
    "print(onw_counter.most_common()[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin to see some words we would expect to denote a negative review, but not so much for the positive reviews. Why might this be the case? What types of tokens are found in positive reviews but not in negative reviews?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*\n",
    "\n",
    ": Firstly, it is observed that positive reviews often include an individual's name or title, such as Edie or Gundam. This is likely due to the fact that people tend to highlight what they appreciate more frequently than what they do not. Therefore, a review is more likely to be positive if it contains these markers.\r\n",
    "\r\n",
    "Tokens that explicitly signify positive reviews, like \"8/10,\" \"7/10,\" \"10/10,\" and \"9/10,\" are naturally more prevalent in positive reviews.\r\n",
    "\r\n",
    "Furthermore, the  positive reviews incorporate a higher frequency of neutral vocabulary compared to emotional language, as opposed to negative reviews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of space separated vocab: 281137\n",
      "['dispels', 'filler;', 'godmother,', 'Shoot', '(Last', 'Hearty', 'who),that', 'Ibsen;', '\"controversial\"', 'replaced', 'god-like', 'Clan.', 'Schnappmann', '(Lemercier', 'Somers', 'entr√©e,', 'True,', 'seven-foot', 'half-hour', 'Phoenixville,', 'Chance.Fox', 'traitorous', 'shimmering,', 'Quite.', 'EVERYBODY---', 'five-scene', 'Simpsons\"),', 'Fawcett...Until', 'Gulf', 'world-weariness', '(peeking', 'jeans,', 'SOON.Each', '(starving', \"Millionaire's\", 'rites.', '19.', 'TV...anything', 'cardboard', 'ending\\x97in', 'Aristotle.', 'was),', 'killings', 'timeless.', 'slathered', 'Ratio', 'life..<br', 'motifs', 'taming', 'nuptials.<br']\n"
     ]
    }
   ],
   "source": [
    "# Lets now make our combined vocabulary\n",
    "space_vocab = list(pos_vocab.union(neg_vocab))\n",
    "print('Length of space separated vocab: {}'.format(len(space_vocab)))\n",
    "print(space_vocab[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at some words from our vocab, what issue do we find by only splitting on spaces?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*\n",
    "\n",
    ": First, there's the issue of tokens being found in a combined form with punctuation. For example, you might have 'Dan.', 'theme).', and 'gimmick!' In these cases, they can be included in the vocabulary as duplicates.\n",
    "\n",
    " Also, as mentioned earlier, HTML Tags and Markup should not be included in the ideal Vocabulary.\n",
    "\n",
    " It is also not appropriate for the vocabulary to contain tokens that combine numbers and alphabets, such as '1977;', '1983.Back', '65-minute', etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, rather than naively splitting on spaces, we can use tools which are informed about English grammar rules to create a cleaner tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bromwell', 'High', 'is', 'a', 'cartoon', 'comedy', '.', 'It', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life', ',', 'such', 'as', '``', 'Teachers', \"''\", '.', 'My', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'Bromwell', 'High', \"'s\", 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', '``', 'Teachers', \"''\", '.', 'The', 'scramble', 'to', 'survive', 'financially', ',', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', 'teachers', \"'\", 'pomp', ',', 'the', 'pettiness', 'of', 'the', 'whole', 'situation', ',', 'all', 'remind', 'me', 'of', 'the', 'schools', 'I', 'knew', 'and', 'their', 'students', '.', 'When', 'I', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school', ',', 'I', 'immediately', 'recalled', '.........', 'at', '..........', 'High', '.', 'A', 'classic', 'line', ':', 'INSPECTOR', ':', 'I', \"'m\", 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers', '.', 'STUDENT', ':', 'Welcome', 'to', 'Bromwell', 'High', '.', 'I', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'Bromwell', 'High', 'is', 'far', 'fetched', '.', 'What', 'a', 'pity', 'that', 'it', 'is', \"n't\", '!']\n",
      "['Story', 'of', 'a', 'man', 'who', 'has', 'unnatural', 'feelings', 'for', 'a', 'pig', '.', 'Starts', 'out', 'with', 'a', 'opening', 'scene', 'that', 'is', 'a', 'terrific', 'example', 'of', 'absurd', 'comedy', '.', 'A', 'formal', 'orchestra', 'audience', 'is', 'turned', 'into', 'an', 'insane', ',', 'violent', 'mob', 'by', 'the', 'crazy', 'chantings', 'of', 'it', \"'s\", 'singers', '.', 'Unfortunately', 'it', 'stays', 'absurd', 'the', 'WHOLE', 'time', 'with', 'no', 'general', 'narrative', 'eventually', 'making', 'it', 'just', 'too', 'off', 'putting', '.', 'Even', 'those', 'from', 'the', 'era', 'should', 'be', 'turned', 'off', '.', 'The', 'cryptic', 'dialogue', 'would', 'make', 'Shakespeare', 'seem', 'easy', 'to', 'a', 'third', 'grader', '.', 'On', 'a', 'technical', 'level', 'it', \"'s\", 'better', 'than', 'you', 'might', 'think', 'with', 'some', 'good', 'cinematography', 'by', 'future', 'great', 'Vilmos', 'Zsigmond', '.', 'Future', 'stars', 'Sally', 'Kirkland', 'and', 'Frederic', 'Forrest', 'can', 'be', 'seen', 'briefly', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "pos_examples_tokenized = [word_tokenize(ex) for ex in pos_examples]\n",
    "neg_examples_tokenized = [word_tokenize(ex) for ex in neg_examples]\n",
    "\n",
    "print(pos_examples_tokenized[0])\n",
    "print(neg_examples_tokenized[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the first example we can see that things like apostrophes, periods, \"n'ts\" and ellipses are better handled.\n",
    "\n",
    "Let's begin defining features for our model. The simplest features are simply if a word exists or not -- however, this is will be very slow if we decide to use the whole vocabulary. Instead, let's create these features for the top 100 most common words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', ',', '.', 'and', 'a', 'of', 'to', 'is', '/', '>', '<', 'br', 'in', 'I', 'it', 'that', \"'s\", 'this', 'was', 'The', 'as', 'with', 'movie', 'for', 'film', ')', '(', 'but', \"n't\", \"''\", '``', 'on', 'you', 'are', 'not', 'have', 'his', 'be', 'he', '!', 'one', 'at', 'by', 'all', 'an', 'who', 'they', 'from', 'like', 'It', 'her', 'so', 'or', 'about', 'has', 'just', 'out', '?', 'do', 'This', 'some', 'good', 'more', 'very', 'would', 'what', 'there', 'up', 'can', 'which', 'when', 'time', 'she', 'had', 'if', 'only', 'really', 'story', 'were', 'their', 'even', 'see', 'no', 'my', 'me', 'does', \"'\", 'did', ':', '-', 'than', '...', 'much', 'been', 'could', 'into', 'get', 'will', 'we', 'other']\n"
     ]
    }
   ],
   "source": [
    "all_tokenized_words = [word for ex in pos_examples_tokenized for word in ex] + \\\n",
    "    [word for ex in neg_examples_tokenized for word in ex]\n",
    "\n",
    "atw_counter = Counter(all_tokenized_words)\n",
    "top100 = [tup[0] for tup in atw_counter.most_common(100)] # A list of the top 100 most frequent word\n",
    "\n",
    "print(top100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following block to define your own features for the NB model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features here\n",
    "\n",
    "def top100_word_features(example): # 100 features, 1 for each word in the top 100 most frequent words\n",
    "    return {word : 1 if word in example else 0 for word in top100}\n",
    "\n",
    "''' Define your own methods here, which take in a single example, and return a feature value (could be a 0/1 truth value, or a count)\n",
    "    Some ideas:\n",
    "        Look at the length of examples, is there a difference between positive and negative examples?\n",
    "        Are there specific words that could be very indiciative? They may not be in the top 100. \n",
    "'''\n",
    "\n",
    "def word_feature(example):\n",
    "    sens_words = [\"happy\", \"exciting\", \"joyful\", \"fantastic\", \"awesome\", \"terrific\", \"sad\", \"disappointing\", \"frustrating\", \"terrible\", \"horrible\"]\n",
    "    return {word : 1 if word in example else 0 for word in sens_words}\n",
    "\n",
    "def length_feature(example):\n",
    "    words = example.split(' ')\n",
    "    feature_dict = {word: 1 if len(word) >= 7 else 0 for word in words}\n",
    "    return feature_dict\n",
    "\n",
    "def score_feature(example):\n",
    "    score_list = [\"0/10\", \"1/10\", \"2/10\", \"3/10\", \"4/10\", \"5/10\", \"6/10\", \"7/10\", \"8/10\", \"9/10\", \"10/10\"]\n",
    "    return {word : 1 if word in score_list else 0 for word in example}\n",
    "\n",
    "def create_feature_dictionary(example):\n",
    "    features = {}\n",
    "    for feat in [top100_word_features, word_feature, length_feature]: #Once you've created your methods, and them to this list\n",
    "        features.update(feat(example))\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've defined our features for our model, we can create our final dataset, which will consist of extracted features and the example label. \n",
    "\n",
    "We'll also create a *validation* split by taking 20% of the training dataset. Remember, we never use the test set to make modeling decisions (in this case, decisions about features). Experiment with multiple models that make use of different combinations of features. Measure their performance on the validation split to figure out which features are the most helpful (use the show_most_informative_features function). When you've found your final model, evaluate its performance on the held out data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "import random\n",
    "\n",
    "# Convert training examples to a set of features.\n",
    "train = [(create_feature_dictionary(ex), 0) for ex in neg_examples] + \\\n",
    "                [(create_feature_dictionary(ex), 1) for ex in pos_examples]\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(train)\n",
    "\n",
    "split_percent = .2\n",
    "\n",
    "cutoff = int(split_percent * len(train))\n",
    "\n",
    "validation_set = train[:cutoff]\n",
    "training_set = train[cutoff:]\n",
    "\n",
    "model = NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.862\n",
      "Most Informative Features\n",
      "                   Avoid = 0                   0 : 1      =     43.5 : 1.0\n",
      "                    4/10 = 0                   0 : 1      =     37.7 : 1.0\n",
      "                     Uwe = 0                   0 : 1      =     27.1 : 1.0\n",
      "                  10/10. = 0                   1 : 0      =     25.6 : 1.0\n",
      "                 Matthau = 1                   1 : 0      =     25.6 : 1.0\n",
      "                  awful. = 0                   0 : 1      =     25.4 : 1.0\n",
      "                Terrible = 1                   0 : 1      =     25.1 : 1.0\n",
      "                  Highly = 0                   1 : 0      =     24.9 : 1.0\n",
      "               Excellent = 1                   1 : 0      =     24.5 : 1.0\n",
      "                 stinker = 1                   0 : 1      =     24.4 : 1.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify.util import accuracy\n",
    "\n",
    "print('Validation accuracy: {}'.format(accuracy(model, validation_set)))\n",
    "model.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the sets of features you've considered, and note down their performance below. What is the final set of features you found?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*\n",
    "\n",
    ": By default, when only the top100_word_features is considered, the validation accuracy stands at 0.6112.\n",
    "\n",
    "Upon applying 'word_feature', which identifies a list of 11 emotional words that can effectively differentiate between positive and negative reviews, the accuracy improved slightly to 0.6348.\n",
    "\n",
    "Additionally, using 'length_feature', which assigns a score to word tokens exceeding seven characters in length, there was a significant increase in accuracy to 0.8616.\n",
    "\n",
    "Finally, we tested the 'score_feature', which we consider a straightforward method to differentiate positive and negative reviews numerically. However, it increased accuracy by only 0.6216.\n",
    "\n",
    "Among the three novel feature combinations, the top validation accuracy was obtained by only adding 'word_feature' and 'length_feature', with a validation accuracy of 0.862. The remaining combinations achieved lower accuracy.\n",
    "\n",
    "Hence, the ultimate feature set I discovered is 'top100_word_features', 'word_feature', and 'length_feature'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, test your model on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Load and process test data\n",
    "pos_test_examples = read_folder(data_dir + 'test/pos/')\n",
    "neg_test_examples = read_folder(data_dir + 'test/neg/')\n",
    "\n",
    "test_set = [(create_feature_dictionary(ex), 0) for ex in neg_test_examples] + \\\n",
    "                [(create_feature_dictionary(ex), 1) for ex in pos_test_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.84004\n"
     ]
    }
   ],
   "source": [
    "print('Test set accuracy: {}'.format(accuracy(model, test_set)))\n",
    "\n",
    "# Note that we're looking at accuracy -- this is not always the most reliable metric and other choices like F1 might be more informative. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e681adf14836894860de42986132a2fbb5bf9e0a673e28b245b6aa439c639a4f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
