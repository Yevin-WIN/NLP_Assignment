{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Homework 11: Syntactic Parsing\n",
    "#### Introduction to Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Yevin, Kim. (kyevin@students.uni-mainz.de)\n",
    "* Yeonwoo, Nam. (yeonam@students.uni-mainz.de)\n",
    "* Hyerin, Seo. (hyseo@students.uni-mainz.de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can reach 20 points on this homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this homework, we will revisit Homework 05: Syntactic Constituency Parsing. In homework 05, we solved the task by transforming it to a sequence labeling task using RNN models. In this homework, we exchange the model type from RNN to transformer models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have questions, you can reach out via mail: minhducbui@uni-mainz.de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "*Task 1:* Explain why there is more tokens than labels! Give two reasons. XX/2\n",
    "\n",
    "*Task 2:* Explain why this is an issue for our task. -> Evaluation: XX/2\n",
    "\n",
    "*Task 3:* Think about other transformations (similar to mine) to solve the task. -> Evaluation: XX/2\n",
    "\n",
    "*Task 4:* Create the corresponding dataset for my solution! -> Evaluation: XX/4\n",
    "\n",
    "*Task 5:* Now initialize the pre-trained model \"distilbert-base-uncased\". Use DistilBertForTokenClassification. -> Evaluation: XX/2\n",
    "\n",
    "*Task 6:* Write the evaluation function which calculates the accuracy per token during the training loop of the Trainer. XX/2\n",
    "\n",
    "*Task 7:* Calculate the token accuracy for the test set! -> Evaluation: XX/2\n",
    "\n",
    "*Task 8:*  Train a non-pre-trained distilbert! Then test the fine-tuned model. -> Evaluation: XX/2\n",
    "\n",
    "*Task 9:* What are your results? Reason, why pre-training helps/does not help on this task. -> Evaluation: XX/2\n",
    "\n",
    "\n",
    "**Total: XX/20**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In homework 05, we converted the Syntactic Constituency Parsing task to a sequence labeling task. Execute the following scripts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\kimye\\anaconda3\\envs\\yevin\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in c:\\users\\kimye\\anaconda3\\envs\\yevin\\lib\\site-packages (from scikit-learn) (1.26.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\kimye\\anaconda3\\envs\\yevin\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\kimye\\anaconda3\\envs\\yevin\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\kimye\\anaconda3\\envs\\yevin\\lib\\site-packages (from scikit-learn) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\kimye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tree:\n",
      "(S (NP-SBJ (DT This)) (VP (VBZ is) (NP-PRD (NNP Japan))) (. ?))\n",
      "\n",
      "Transformed Tree:\n",
      "  (w1 (1, S))\n",
      "  (w2 (1, VP))\n",
      "  (w3 (-1, S))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "    \n",
    "# Download the Penn Treebank datas\n",
    "treebank_trees = treebank.parsed_sents()\n",
    "\n",
    "# Get an example\n",
    "sorted_treebank_trees = sorted(treebank_trees, key=lambda tree: len(tree.leaves()))\n",
    "test_example = sorted_treebank_trees[31].copy()\n",
    "\n",
    "# Function to get the common ancestors and their count using the relative scale\n",
    "def relative_scale_encoding(tree):\n",
    "    # Initialize an empty list to store the relative scale encoding for each word pair\n",
    "    result = []\n",
    "\n",
    "    # Save the previous number of common ancestors\n",
    "    prev_common_ancestors = 0\n",
    "\n",
    "    # Iterate over each pair of consecutive leaves in the tree\n",
    "    for i in range(len(tree.leaves()) - 1):\n",
    "        # Extract the current pair of words\n",
    "        word1, word2 = tree.leaves()[i], tree.leaves()[i + 1]\n",
    "\n",
    "        # Get the tree positions of the leaves for both words\n",
    "        # E.g. (1, 0, 0) and (1, 1, 0)\n",
    "        path1 = tree.leaf_treeposition(i)\n",
    "        path2 = tree.leaf_treeposition(i + 1)\n",
    "\n",
    "        # Find the common path between the tree positions\n",
    "        # E.g. (1, 0, 0) and (1, 1, 0) -> [1]\n",
    "        common_path = []\n",
    "        for j, (p1, p2) in enumerate(zip(path1, path2)):\n",
    "            if p1 == p2:\n",
    "                common_path.append(p1)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # Extract the common ancestor of the current word pair\n",
    "        common_ancestor = tree[common_path]\n",
    "\n",
    "        # Calculate the number of common ancestors + 1 (including the leaves themselves)\n",
    "        common_ancestors_count = len(common_path) + 1\n",
    "\n",
    "        # Calculate the relative scale by subtracting the previous common ancestors count\n",
    "        relative_scale = common_ancestors_count - prev_common_ancestors\n",
    "\n",
    "        # Append the result for the current word pair as a list of [relative_scale, common_ancestor_label]\n",
    "        result.append([str(relative_scale), common_ancestor.label()])\n",
    "\n",
    "        # Update the previous common ancestors count for the next iteration\n",
    "        prev_common_ancestors = common_ancestors_count\n",
    "\n",
    "    # Return the list of relative scale encodings for each word pair\n",
    "    return result\n",
    "\n",
    "\n",
    "# Get the common ancestors and their count for each pair of adjacent words\n",
    "encoding = relative_scale_encoding(test_example)\n",
    "\n",
    "# Print the transformed tree in the specified format\n",
    "print(\"Original Tree:\")\n",
    "print(test_example)\n",
    "print(\"\\nTransformed Tree:\")\n",
    "for i, (n_i, c_i) in enumerate(encoding, start=1):\n",
    "    print(f\"  (w{i} ({n_i}, {c_i}))\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You might have to install sklearn first. Use the following command and execute it in another cell:\n",
    "# !pip install -U scikit-learn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "sentences = []\n",
    "# sentences = Holds the sentence of the tree\n",
    "\n",
    "encodings = []\n",
    "# encodings = Holds the label pairs with a dummy variable at the end\n",
    "# Hint: I would encode the labels with the dummy variable as ['-1', 'DUMMY']\n",
    "# For our test example: [['1', 'S'], ['1', 'VP'], ['-1', 'S'], ['-1', 'DUMMY']]\n",
    "\n",
    "class_labels = []\n",
    "# labels = Holds the class indices, e.g. [768, 829, 165, 72] for one sentence (each token has one)\n",
    "# Hint: I would transform each pair to a string by joining them with \"_\"\n",
    "# E.g. '1_S', '1_VP', '-1_S', '-1_DUMMY', ...\n",
    "# Collect all labels and fit your LabelEncoder on all labels (use .fit() method)\n",
    "# Then transform each example with your LabelEncoder (use .transform() method)\n",
    "# For our Test Example, I got: [768 829 165 72]\n",
    "\n",
    "\n",
    "# Sentences\n",
    "sentences = [tree.leaves() for tree in treebank_trees]\n",
    "\n",
    "# Encodings\n",
    "for tree in treebank_trees:\n",
    "    encoding = relative_scale_encoding(tree)\n",
    "    # Add a dummy target for the last token\n",
    "    encoding.append([\"-1\", \"DUMMY\"])\n",
    "    encodings += [encoding]\n",
    "\n",
    "# Class Labels\n",
    "def transform_to_string(encoding):\n",
    "    return [\"_\".join(pair) for pair in encoding]\n",
    "label_encoder = LabelEncoder()\n",
    "targets = [transform_to_string(encoding) for encoding in encodings]\n",
    "flattened = []\n",
    "for target in targets:\n",
    "    flattened += target\n",
    "label_encoder.fit(flattened)\n",
    "class_labels = [label_encoder.transform(transform_to_string(encoding)) for encoding in encodings]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we converted the dataset into two components: _sentences_ which holds the sentence of each tree and _class_labels_ which holds the label for each word in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences (length: 12): ['A', 'Lorillard', 'spokewoman', 'said', ',', '``', 'This', 'is', 'an', 'old', 'story', '.']\n",
      "Labels (length: 12): [859 494 165 829 628 628 768 829 688 493 324  72]\n"
     ]
    }
   ],
   "source": [
    "test_index = 7\n",
    "print(\"Sentences (length: {}): {}\".format(len(sentences[test_index]), sentences[test_index]))\n",
    "print(\"Labels (length: {}): {}\".format(len(class_labels[test_index]), class_labels[test_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained DistilBert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will experiment with \"DistilBert\" (https://arxiv.org/pdf/1910.01108.pdf), which is a small BERT-like model, i.e. an encoder-only model. Load the tokenizer of this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForTokenClassification\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:\n",
      "A Lorillard spokewoman said , `` This is an old story .\n",
      "\n",
      "Label (length=12):\n",
      "[859 494 165 829 628 628 768 829 688 493 324  72]\n",
      "\n",
      "Tokenizer Output (length=18): \n",
      "{'input_ids': [101, 1037, 18669, 17305, 2094, 3764, 10169, 2056, 1010, 1036, 1036, 2023, 2003, 2019, 2214, 2466, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage of the tokenizer:\n",
    "\n",
    "test_sentence = \" \".join(sentences[test_index])\n",
    "print(\"Sentence:\\n{}\\n\".format(test_sentence))\n",
    "print(\"Label (length={}):\\n{}\\n\".format(len(class_labels[test_index]), class_labels[test_index]))\n",
    "print(\"Tokenizer Output (length={}): \\n{}\".format(len(tokenizer(test_sentence).input_ids), tokenizer(test_sentence)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an issue with the task now: Our tokenizer creates more tokens than labels! Our task assumes one label per word/token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1**: Explain why there is more tokens than labels! Give two reasons. (2P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ": First, during the process of tokenizing a sentence, it is possible that a compound word that was classified as a single word was broken down into two or more tokens. For example, 'spokewoman' may have been broken down into 'spoke' and 'woman'. Also, depending on the tokenizer, special tokens may have been added to signify the beginning and end of a sequence or to handle punctuation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2:** Explain why this is an issue for our task. (2P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ": When label lengths and token lengths do not match, a dimensional mismatch occurs between the model's input and output. Since the transformer model relies on attention mechanisms that operate on fixed-size sequences, this mismatch is unclear how the model should handle it during training and prediction. It can also cause problems with loss computation, which can hinder the model learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve this task, I propose the following solution:\n",
    "\n",
    "- We only take into consideration the last token of each word to calculate our loss function and therefore updating our model:\n",
    "   - e.g. \"spokewomen\" is splitted into two tokens (3764, 10169). In this case, we only consider the prediction for the last token (10169) into our loss function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3:** Think about other transformations (similar to mine) to solve the task. (2P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ": Using the DistilBERT tokenizer for labels is crucial for maintaining consistency in the tokenization process with the input sentences. The labels are initially tokenized to break them into individual tokens, aligning them with the model's understanding. Each resulting token is then matched with its corresponding index in the DistilBERT vocabulary, assigning a specific position within the model's embedding space. The labels are encoded based on these indices, creating an organized list or array. To address additional tokens like [CLS] and [SEP] introduced by the BERT tokenizer, masking is applied to prioritize essential information. Applying these encoded labels during model training ensures that both sentence and label information are considered concurrently, improving the model's performance on tasks related to the provided labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how do we ignore predictions of _non-last_ tokens? In Huggingface, the **class index \"-100\" will be ignored in the calculation of the loss function**. So for every token, that should not be considered, we will add a -100 to the corresponding index in class_labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4:** Create the corresponding dataset for my solution! (4P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "'''\n",
    "\n",
    "Your code here.\n",
    "\n",
    "Create PyTorch Dataset for my solution. Only take into consideration the last token of \n",
    "each word to calculate our loss function and therefore updating our model.\n",
    "\n",
    "E..g. \"spokewomen\" is splitted into two tokens (3764, 10169). \n",
    "In this case, we only consider the prediction for the last token (10169) \n",
    "into our loss function!\n",
    "\n",
    "Hint: I used special token [SEP] (token_id = 102) to track boundaries of each word, e.g.\n",
    "-> \"[SEP]\".join(sentences[test_index])\n",
    "-> each token_id before 102 is the last token of a word\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "class TreeDatasetGloVeIndexed(Dataset):\n",
    "    def __init__(self, sentences, class_labels):\n",
    "        self.sentences = sentences\n",
    "        self.class_labels = class_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        labels = self.class_labels[idx]\n",
    "\n",
    "        # Join the tokens to form the input sentence & Tokenize the sentence\n",
    "        input_sentence = \"[SEP]\".join(sentence)\n",
    "        tokenized_output = tokenizer(input_sentence)\n",
    "        \n",
    "        # Get the last token for each word based on [SEP] token\n",
    "        current_index = 0\n",
    "        last_token_indices = []\n",
    "        \n",
    "        for i, token in enumerate(tokenized_output['input_ids']):\n",
    "            if token != 102:\n",
    "                current_index += 1\n",
    "            else:\n",
    "                last_token_indices.append(current_index-1)\n",
    "\n",
    "        last_token_labels = []\n",
    "\n",
    "        last_token_embeddings = [token for token in tokenized_output['input_ids'] if token != 102]\n",
    "\n",
    "        idx = 0\n",
    "        \n",
    "        for i in range(len(last_token_embeddings)):\n",
    "            if i < last_token_indices[idx]:\n",
    "                last_token_labels.append(-100)\n",
    "            else:\n",
    "                last_token_labels.append(labels[idx])\n",
    "                idx += 1\n",
    "                \n",
    "        \n",
    "        indices_tensor = torch.tensor(last_token_embeddings)\n",
    "        class_label = torch.tensor(last_token_labels)\n",
    "        \n",
    "        # Output should be a dictionary with keys input_ids and labels\n",
    "        # Both are tensors (see assert function)\n",
    "        return {\"input_ids\": indices_tensor, \"labels\": class_label}\n",
    "\n",
    "\n",
    "# We use a batch size of 1 only.\n",
    "batch_size = 1\n",
    "\n",
    "# Train\n",
    "train_sentences = sentences[:3000]\n",
    "train_labels = class_labels[:3000]\n",
    "train_tree_dataset = TreeDatasetGloVeIndexed(train_sentences, train_labels)\n",
    "\n",
    "# Dev/Val\n",
    "dev_sentences = sentences[3000:3100]\n",
    "dev_labels = class_labels[3000:3100]\n",
    "dev_tree_dataset = TreeDatasetGloVeIndexed(dev_sentences, dev_labels)\n",
    "\n",
    "# Test\n",
    "test_sentences = sentences[3139:]\n",
    "test_labels = class_labels[3139:]\n",
    "test_tree_dataset = TreeDatasetGloVeIndexed(test_sentences, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This was my assert. Ignore, if you used a slightly different strategy.\n",
    "# Multiple strategies could be valid.\n",
    "\n",
    "assert torch.equal(train_tree_dataset[0][\"input_ids\"], torch.tensor([  101,  5578, 19354,  7520,  1010,  6079,  2086,  2214,  1010,  2097,\n",
    "          3693,  1996,  2604,  2004,  1037,  3904,  2595,  8586, 28546,  2472,\n",
    "         13292,  1012,  2756,  1012]))\n",
    "\n",
    "assert torch.equal(train_tree_dataset[0][\"labels\"], torch.tensor([-100, 1009, -100,   90,  494,  855,   61,   90,  165,  829,  829,  675,\n",
    "         196,  729,  675, -100, -100, -100,  482,  257, -100,  713,  279,   72]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5**: Now initialize the **pre-trained** model \"distilbert-base-uncased\". Use DistilBertForTokenClassification. (2P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForTokenClassification\n",
    "\n",
    "# Convert NumPy arrays to lists\n",
    "train_labels_list = [label.tolist() for label in train_labels]\n",
    "dev_labels_list = [label.tolist() for label in dev_labels]\n",
    "test_labels_list = [label.tolist() for label in test_labels]\n",
    "\n",
    "# Combine all labels and find unique classes\n",
    "all_labels = train_labels_list + dev_labels_list + test_labels_list\n",
    "num_classes = len(set(tuple(label) for label in all_labels))\n",
    "\n",
    "# Look up how you can initialize the pre-trained model with the correct number of classes!\n",
    "model = DistilBertForTokenClassification.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    num_labels=num_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6:** Write the evaluation function which calculates the accuracy per token during the training loop of the Trainer. (2P)\n",
    "\n",
    "_Hint: Ignore labels with -100!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForTokenClassification\n",
    "\n",
    "'''\n",
    "\n",
    "Your code here.\n",
    "\n",
    "Write the evaluation function which calculates the accuracy per token.\n",
    "\n",
    "Hint: Ignore labels with -100! Also test this function by executing the training loop.\n",
    "\n",
    "'''\n",
    "\n",
    "def compute_token_accuracy(preds):\n",
    "    output, labels = torch.from_numpy(preds.predictions), torch.from_numpy(preds.label_ids)\n",
    "    active_loss = labels != -100\n",
    "    logits_argmax = torch.argmax(output, dim=2)\n",
    "    \n",
    "    logits_flat = logits_argmax[active_loss]\n",
    "    labels_flat = labels[active_loss]\n",
    "    \n",
    "    correct_tokens = torch.sum(logits_flat == labels_flat).item()\n",
    "    total_tokens = len(labels_flat)\n",
    "    \n",
    "    return {\"token_accuracy\": correct_tokens / total_tokens}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train the model:\n",
    "\n",
    "**Important: You can reduce the num_train_epochs to reduce training speed!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='940' max='940' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [940/940 34:52, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.661800</td>\n",
       "      <td>1.226698</td>\n",
       "      <td>0.746503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.217600</td>\n",
       "      <td>1.019712</td>\n",
       "      <td>0.765734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.014800</td>\n",
       "      <td>0.950359</td>\n",
       "      <td>0.778409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.948400</td>\n",
       "      <td>0.908865</td>\n",
       "      <td>0.792395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.658300</td>\n",
       "      <td>0.907179</td>\n",
       "      <td>0.790210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.639900</td>\n",
       "      <td>0.887640</td>\n",
       "      <td>0.802010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.655800</td>\n",
       "      <td>0.915546</td>\n",
       "      <td>0.788024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.559600</td>\n",
       "      <td>0.886522</td>\n",
       "      <td>0.799388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.439300</td>\n",
       "      <td>0.864333</td>\n",
       "      <td>0.807255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.447300</td>\n",
       "      <td>0.865639</td>\n",
       "      <td>0.804196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.435100</td>\n",
       "      <td>0.829484</td>\n",
       "      <td>0.819056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.355600</td>\n",
       "      <td>0.859224</td>\n",
       "      <td>0.815122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.319000</td>\n",
       "      <td>0.850688</td>\n",
       "      <td>0.817308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.330100</td>\n",
       "      <td>0.838602</td>\n",
       "      <td>0.823427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.322600</td>\n",
       "      <td>0.834250</td>\n",
       "      <td>0.822115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.252000</td>\n",
       "      <td>0.845002</td>\n",
       "      <td>0.821678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.242700</td>\n",
       "      <td>0.852810</td>\n",
       "      <td>0.817745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.246700</td>\n",
       "      <td>0.844580</td>\n",
       "      <td>0.818619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (688 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=940, training_loss=0.5288104270366912, metrics={'train_runtime': 2093.464, 'train_samples_per_second': 7.165, 'train_steps_per_second': 0.449, 'total_flos': 296767600186656.0, 'train_loss': 0.5288104270366912, 'epoch': 5.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./sequence_labeling_model\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=3,\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Create data collator for token classification\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "# Create Trainer with the custom metric\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tree_dataset,\n",
    "    eval_dataset=dev_tree_dataset,\n",
    "    compute_metrics=compute_token_accuracy,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 7**: Calculate the token accuracy for the test set! (2P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='775' max='775' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [775/775 01:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Accuracy on Test Set: 0.8429\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Your code here.\n",
    "\n",
    "Calculate the token accuracy for the test set!\n",
    "\n",
    "'''\n",
    "test_results = trainer.evaluate(test_tree_dataset)\n",
    "\n",
    "# Extract token accuracy from the evaluation results\n",
    "token_accuracy = test_results[\"eval_token_accuracy\"]\n",
    "\n",
    "print(f\"Token Accuracy on Test Set: {token_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model looks pretty good! As a last test, let us train an non-pre-trained distilbert and see how much pre-training is helping us on this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 8:** Train a non-pre-trained distilbert! Then test the fine-tuned model. (2P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='940' max='940' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [940/940 33:38, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.988500</td>\n",
       "      <td>2.103124</td>\n",
       "      <td>0.583042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.925000</td>\n",
       "      <td>1.504329</td>\n",
       "      <td>0.694930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.442300</td>\n",
       "      <td>1.288113</td>\n",
       "      <td>0.737762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.306100</td>\n",
       "      <td>1.173345</td>\n",
       "      <td>0.736014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.011100</td>\n",
       "      <td>1.067780</td>\n",
       "      <td>0.753059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.912600</td>\n",
       "      <td>1.005751</td>\n",
       "      <td>0.766608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.907800</td>\n",
       "      <td>0.947636</td>\n",
       "      <td>0.781031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.797800</td>\n",
       "      <td>0.926845</td>\n",
       "      <td>0.795455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.643000</td>\n",
       "      <td>0.882969</td>\n",
       "      <td>0.803759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.621500</td>\n",
       "      <td>0.885691</td>\n",
       "      <td>0.802448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.599200</td>\n",
       "      <td>0.847776</td>\n",
       "      <td>0.802010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.503500</td>\n",
       "      <td>0.866724</td>\n",
       "      <td>0.808566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.472800</td>\n",
       "      <td>0.857190</td>\n",
       "      <td>0.809878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.473900</td>\n",
       "      <td>0.838193</td>\n",
       "      <td>0.814685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.448700</td>\n",
       "      <td>0.824124</td>\n",
       "      <td>0.809003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.372800</td>\n",
       "      <td>0.825303</td>\n",
       "      <td>0.815997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.358700</td>\n",
       "      <td>0.840536</td>\n",
       "      <td>0.814248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.369100</td>\n",
       "      <td>0.830245</td>\n",
       "      <td>0.815122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (688 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='775' max='775' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [775/775 01:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Accuracy on Test Set: 0.8427457882826251\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Your code here.\n",
    "\n",
    "Hint: Basically copy+paste the above pipeline.\n",
    "\n",
    "'''\n",
    "\n",
    "# Define your non-pre-trained DistilBERT model\n",
    "model = DistilBertForTokenClassification.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    num_labels=num_classes\n",
    ")\n",
    "\n",
    "# Define the tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Train\n",
    "train_sentences = sentences[:3000]\n",
    "train_labels = class_labels[:3000]\n",
    "train_tree_dataset = TreeDatasetGloVeIndexed(train_sentences, train_labels)\n",
    "\n",
    "# Dev/Val\n",
    "dev_sentences = sentences[3000:3100]\n",
    "dev_labels = class_labels[3000:3100]\n",
    "dev_tree_dataset = TreeDatasetGloVeIndexed(dev_sentences, dev_labels)\n",
    "\n",
    "# Test\n",
    "test_sentences = sentences[3139:]\n",
    "test_labels = class_labels[3139:]\n",
    "test_tree_dataset = TreeDatasetGloVeIndexed(test_sentences, test_labels)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./sequence_labeling_model\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=3,\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Create data collator for token classification\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "# Create Trainer with the custom metric\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tree_dataset,\n",
    "    eval_dataset=dev_tree_dataset,\n",
    "    compute_metrics=compute_token_accuracy,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_results = trainer.evaluate(test_tree_dataset)\n",
    "\n",
    "# Print the token accuracy for the test set\n",
    "print(\"Token Accuracy on Test Set:\", test_results[\"eval_token_accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 9:** What are your results? Reason, why pre-training helps/does not help on this task. (2P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ": In conclusion, there was no significant difference in performance between the trained (pre-trained) model and the untrained (non-pre-trained) model. From these results, it can be inferred that the pre-trained model did not provide substantial assistance in capturing the specific details of the current task. As an interpretation of these findings, it is possible that the dataset used for pre-training the model was either unrelated or insufficiently representative of the current task. For instance, although pre-training may have been helpful in learning general language features, it might not have adequately captured the domain-specific information crucial for the present task. Alternatively, the small size of the dataset could have made it challenging for the model to undergo sufficient training.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
